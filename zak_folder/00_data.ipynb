{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import GetOldTweets3 as got\n",
    "import time\n",
    "from calendar import Calendar, monthrange\n",
    "from datetime import datetime, timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to request tweets within a certain time period\n",
    "def get_tweets(query, since, until, max_tweets,location, radius):\n",
    "    # Set our tweet criteria using GetOldTweets3\n",
    "    tweetCriteria = got.manager.TweetCriteria()\\\n",
    "                                            .setQuerySearch(query)\\\n",
    "                                            .setSince(since)\\\n",
    "                                            .setUntil(until)\\\n",
    "                                            .setMaxTweets(max_tweets)\\\n",
    "                                            .setNear(location)\\\n",
    "                                            .setWithin(radius)\n",
    "    \n",
    "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(tweet):\n",
    "    total_list = []\n",
    "    for i in range(len(tweet)):\n",
    "        my_dict = {\n",
    "            \"id\" : tweet[i].id,\n",
    "            \"text\" : tweet[i].text,\n",
    "            \"date\" : tweet[i].date,\n",
    "            \"retweets\" : tweet[i].retweets,\n",
    "            \"favorites\" : tweet[i].favorites,\n",
    "            \"mentions\" : tweet[i].mentions,\n",
    "            \"hashtags\" : tweet[i].hashtags,\n",
    "            \"geo\" : tweet[i].geo  \n",
    "        }\n",
    "        total_list.append(my_dict)\n",
    "    return pd.DataFrame(total_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "since_list = [\"2020-03-01\",\"2020-03-08\", \"2020-03-15\", \"2020-03-22\", \"2020-03-29\",\n",
    "             \"2020-04-05\", \"2020-04-12\", \"2020-04-19\", \"2020-04-26\", \"2020-05-03\",\n",
    "             \"2020-05-10\", \"2020-05-17\", \"2020-05-24\", \"2020-05-31\" ,\"2020-06-07\",\n",
    "             \"2020-06-14\", \"2020-06-21\", \"2020-06-28\", \"2020-07-05\", \"2020-07-12\",\n",
    "             \"2020-07-19\", \"2020-07-26\", \"2020-08-02\", \"2020-08-09\", \"2020-08-16\",\n",
    "             \"2020-08-23\", \"2020-08-30\"]\n",
    "\n",
    "until_list = [\"2020-03-08\", \"2020-03-15\", \"2020-03-22\", \"2020-03-29\",\"2020-04-05\", \n",
    "              \"2020-04-12\", \"2020-04-19\", \"2020-04-26\", \"2020-05-03\",\"2020-05-10\", \n",
    "              \"2020-05-17\", \"2020-05-24\", \"2020-05-31\" ,\"2020-06-07\",\"2020-06-14\", \n",
    "              \"2020-06-21\", \"2020-06-28\", \"2020-07-05\", \"2020-07-12\",\"2020-07-19\", \n",
    "              \"2020-07-26\", \"2020-08-02\", \"2020-08-09\", \"2020-08-16\",\"2020-08-23\", \n",
    "              \"2020-08-30\", \"2020-09-06\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_tweets(region, query, since, until,\n",
    "                max_tweets, location, radius):\n",
    "    t0 = time.time() \n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(since)):\n",
    "        x = get_tweets(query=query,\n",
    "                   since=since[i],\n",
    "                   until=until[i],\n",
    "                   max_tweets = max_tweets,\n",
    "                   location = location,\n",
    "                   radius = radius\n",
    "                  )\n",
    "        df = pd.concat([df,create_df(x)])\n",
    "        print(f\"Index {i}: week {since[i]} complete at {t0 - time.time()} seconds\")\n",
    "        #if i < len(since)-1:\n",
    "            #time.sleep(60)\n",
    "    print(f\"Final run time: {t0 - time.time()} seconds\")\n",
    "    df[\"region\"] = region\n",
    "    df.to_csv(f\"{region}_data.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-03-01 complete at -0.6266632080078125 seconds\n",
      "Index 1: week 2020-03-08 complete at -1.0898234844207764 seconds\n",
      "Index 2: week 2020-03-15 complete at -1.644557237625122 seconds\n",
      "Index 3: week 2020-03-22 complete at -2.1415131092071533 seconds\n",
      "Index 4: week 2020-03-29 complete at -2.680084705352783 seconds\n",
      "Index 5: week 2020-04-05 complete at -3.213817596435547 seconds\n",
      "Index 6: week 2020-04-12 complete at -3.65350604057312 seconds\n",
      "Index 7: week 2020-04-19 complete at -4.231722354888916 seconds\n",
      "Index 8: week 2020-04-26 complete at -4.7804038524627686 seconds\n",
      "Index 9: week 2020-05-03 complete at -5.492872714996338 seconds\n",
      "Index 10: week 2020-05-10 complete at -6.142607688903809 seconds\n",
      "Index 11: week 2020-05-17 complete at -6.7705466747283936 seconds\n",
      "Index 12: week 2020-05-24 complete at -7.310587406158447 seconds\n",
      "Index 13: week 2020-05-31 complete at -8.01098108291626 seconds\n",
      "Index 14: week 2020-06-07 complete at -8.490058660507202 seconds\n",
      "Index 15: week 2020-06-14 complete at -9.068986415863037 seconds\n",
      "Index 16: week 2020-06-21 complete at -9.600775957107544 seconds\n",
      "Index 17: week 2020-06-28 complete at -10.076022148132324 seconds\n",
      "Index 18: week 2020-07-05 complete at -10.63760232925415 seconds\n",
      "Index 19: week 2020-07-12 complete at -11.178400754928589 seconds\n",
      "Index 20: week 2020-07-19 complete at -11.685871839523315 seconds\n",
      "Index 21: week 2020-07-26 complete at -12.289175033569336 seconds\n",
      "Index 22: week 2020-08-02 complete at -12.890238285064697 seconds\n",
      "Index 23: week 2020-08-09 complete at -13.466957330703735 seconds\n",
      "Index 24: week 2020-08-16 complete at -14.043758630752563 seconds\n",
      "Index 25: week 2020-08-23 complete at -14.684549570083618 seconds\n",
      "Index 26: week 2020-08-30 complete at -15.212433815002441 seconds\n",
      "Final run time: -15.213431358337402 seconds\n"
     ]
    }
   ],
   "source": [
    "dallas_df = all_tweets(\n",
    "                region =  \"Dallas_Fort_Worth\",\n",
    "                query = 'covid OR coronavirus',\n",
    "                since = since_list,\n",
    "                until = until_list,\n",
    "                max_tweets = 1000,\n",
    "                location = \"32.842557, -96.020077\",\n",
    "                radius = \"100km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>retweets</th>\n",
       "      <th>favorites</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>geo</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1236441362985336844</td>\n",
       "      <td>I would be less worried about Hitler, than I a...</td>\n",
       "      <td>2020-03-07 23:59:27+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1236441280625782785</td>\n",
       "      <td>The cashier/waiter fucked up 4 orders in a row...</td>\n",
       "      <td>2020-03-07 23:59:07+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1236441216880799744</td>\n",
       "      <td>I really be breaking my own heart sometimes</td>\n",
       "      <td>2020-03-07 23:58:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1236441212070043648</td>\n",
       "      <td>Way to go Winter Color Guard, 2nd place at Ame...</td>\n",
       "      <td>2020-03-07 23:58:51+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>@CharlieGarciaBA @YISDFineArts</td>\n",
       "      <td>#bigredpride</td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1236441125784899585</td>\n",
       "      <td>I have all the symptoms of the Coronavirus so ...</td>\n",
       "      <td>2020-03-07 23:58:31+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1302396074867322880</td>\n",
       "      <td>i hate this part on my game. ugh</td>\n",
       "      <td>2020-09-05 23:59:56+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1302396067401605123</td>\n",
       "      <td>2 new #COVID19 deaths were reported in #DoñaAn...</td>\n",
       "      <td>2020-09-05 23:59:54+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>#COVID19 #Do</td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1302396030043009024</td>\n",
       "      <td>Lorraine boutta go off this year! Goodluck thi...</td>\n",
       "      <td>2020-09-05 23:59:45+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>@lorraineemariee</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302395976741801984</td>\n",
       "      <td>@Mafedelregon</td>\n",
       "      <td>2020-09-05 23:59:32+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@Mafedelregon</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1302395962367848449</td>\n",
       "      <td></td>\n",
       "      <td>2020-09-05 23:59:29+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>El_Paso</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0   1236441362985336844  I would be less worried about Hitler, than I a...   \n",
       "1   1236441280625782785  The cashier/waiter fucked up 4 orders in a row...   \n",
       "2   1236441216880799744        I really be breaking my own heart sometimes   \n",
       "3   1236441212070043648  Way to go Winter Color Guard, 2nd place at Ame...   \n",
       "4   1236441125784899585  I have all the symptoms of the Coronavirus so ...   \n",
       "..                  ...                                                ...   \n",
       "0   1302396074867322880                   i hate this part on my game. ugh   \n",
       "1   1302396067401605123  2 new #COVID19 deaths were reported in #DoñaAn...   \n",
       "2   1302396030043009024  Lorraine boutta go off this year! Goodluck thi...   \n",
       "3   1302395976741801984                                      @Mafedelregon   \n",
       "4   1302395962367848449                                                      \n",
       "\n",
       "                        date  retweets  favorites  \\\n",
       "0  2020-03-07 23:59:27+00:00         0          0   \n",
       "1  2020-03-07 23:59:07+00:00         0          1   \n",
       "2  2020-03-07 23:58:52+00:00         0          1   \n",
       "3  2020-03-07 23:58:51+00:00         1         16   \n",
       "4  2020-03-07 23:58:31+00:00         0          0   \n",
       "..                       ...       ...        ...   \n",
       "0  2020-09-05 23:59:56+00:00         0          0   \n",
       "1  2020-09-05 23:59:54+00:00         0          0   \n",
       "2  2020-09-05 23:59:45+00:00         1          7   \n",
       "3  2020-09-05 23:59:32+00:00         0          0   \n",
       "4  2020-09-05 23:59:29+00:00         0         28   \n",
       "\n",
       "                          mentions      hashtags geo   region  \n",
       "0                                                     El_Paso  \n",
       "1                                                     El_Paso  \n",
       "2                                                     El_Paso  \n",
       "3   @CharlieGarciaBA @YISDFineArts  #bigredpride      El_Paso  \n",
       "4                                                     El_Paso  \n",
       "..                             ...           ...  ..      ...  \n",
       "0                                                     El_Paso  \n",
       "1                                   #COVID19 #Do      El_Paso  \n",
       "2                 @lorraineemariee                    El_Paso  \n",
       "3                    @Mafedelregon                    El_Paso  \n",
       "4                                                     El_Paso  \n",
       "\n",
       "[135 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32.7767° N, 96.7970° W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-03-01 complete at -55.957351207733154 seconds\n",
      "Index 1: week 2020-03-08 complete at -113.26359391212463 seconds\n",
      "Index 2: week 2020-03-15 complete at -171.1200361251831 seconds\n",
      "Index 3: week 2020-03-22 complete at -230.6855230331421 seconds\n",
      "Index 4: week 2020-03-29 complete at -292.207640171051 seconds\n",
      "Index 5: week 2020-04-05 complete at -350.4829671382904 seconds\n",
      "Index 6: week 2020-04-12 complete at -401.42146396636963 seconds\n",
      "Final run time: -401.42163825035095 seconds\n"
     ]
    }
   ],
   "source": [
    "dallas_df = all_tweets(\n",
    "                region = \"Dallas_Aug30\",\n",
    "                query = ' ',\n",
    "                since = [\"2020-08-30\", \"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\"],\n",
    "                until = [\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\",\"2020-09-06\"],\n",
    "                max_tweets = 2000,\n",
    "                location = \"32.7767, -96.7970\",\n",
    "                radius = \"10mi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dallas_df['text'].str.contains('covid|coronavirus', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0036"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18/5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-03-01 complete at -53.57201910018921 seconds\n",
      "Index 1: week 2020-03-08 complete at -107.55848979949951 seconds\n",
      "Index 2: week 2020-03-15 complete at -160.12515211105347 seconds\n",
      "Index 3: week 2020-03-22 complete at -217.51840496063232 seconds\n",
      "Index 4: week 2020-03-29 complete at -276.5529308319092 seconds\n",
      "Index 5: week 2020-04-05 complete at -342.2290949821472 seconds\n",
      "Index 6: week 2020-04-12 complete at -392.8458981513977 seconds\n",
      "Final run time: -392.8460519313812 seconds\n"
     ]
    }
   ],
   "source": [
    "ftworth_df = all_tweets(\n",
    "                region =  \"Fort_Worth_Aug30\",\n",
    "                query = ' ',\n",
    "                since = [\"2020-08-30\", \"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\"],\n",
    "                until = [\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\",\"2020-09-06\"],\n",
    "                max_tweets = 2000,\n",
    "                location = \"Fort Worth\",\n",
    "                radius = \"15mi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftworth_df['text'].str.contains('covid|coronavirus', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-03-01 complete at -11.359208822250366 seconds\n",
      "Index 1: week 2020-03-08 complete at -23.283597946166992 seconds\n",
      "Index 2: week 2020-03-15 complete at -37.22045183181763 seconds\n",
      "Index 3: week 2020-03-22 complete at -49.29528188705444 seconds\n",
      "Index 4: week 2020-03-29 complete at -61.68601894378662 seconds\n",
      "Index 5: week 2020-04-05 complete at -119.67050695419312 seconds\n",
      "Index 6: week 2020-04-12 complete at -175.05471897125244 seconds\n",
      "Final run time: -175.05486679077148 seconds\n"
     ]
    }
   ],
   "source": [
    "waco_df = all_tweets(\n",
    "                region =  \"Waco_Aug30\",\n",
    "                query = ' ',\n",
    "                since = [\"2020-08-30\", \"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\"],\n",
    "                until = [\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\",\"2020-09-06\"],\n",
    "                max_tweets = 2000,\n",
    "                location = \"31.5493, -97.1467\",\n",
    "                radius = \"10mi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waco_df['text'].str.contains('covid|coronavirus', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waco_df['text'].str.contains('hoax', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Timing with the YouTube doctors and the CDC trickling out the truth now. It sure would be nice to have one of these Republican leaders really lay into the hoaxes that the media just eats up. You know for America and all.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Shut up! Benghazi was a hoax. We wasted 4 years, millions of $, several hearings to have a report teleased that basically said, nevermind. There was no way it could have been prevented. Anybody with a brain knows, just some wont admit it.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(waco_df[waco_df['text'].str.contains('hoax', flags=re.IGNORECASE, regex=True)]['text'][348])\n",
    "print()\n",
    "display(waco_df[waco_df['text'].str.contains('hoax', flags=re.IGNORECASE, regex=True)]['text'][1379])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go get ya flu shot! #flushot #protectyourlovedones #notahoax #yesimaredsoxfan'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'More feat mongering #PlandemicHoax'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How will they join something they think is a hoax?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#NotTheNews Trump Reveals Einstein Hoax'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dallas_df[dallas_df['text'].str.contains('hoax', flags=re.IGNORECASE, regex=True)]['text'][145])\n",
    "print()\n",
    "display(dallas_df[dallas_df['text'].str.contains('hoax', flags=re.IGNORECASE, regex=True)]['text'][327])\n",
    "print()\n",
    "display(dallas_df[dallas_df['text'].str.contains('hoax', flags=re.IGNORECASE, regex=True)]['text'][1318])\n",
    "print()\n",
    "display(dallas_df[dallas_df['text'].str.contains('hoax', flags=re.IGNORECASE, regex=True)]['text'][1361])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-13-16c22a14ab8a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-16c22a14ab8a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    32.3513° N, 95.3011° W\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "32.3513° N, 95.3011° W\n",
    "8mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-03-01 complete at -10.523451328277588 seconds\n",
      "Index 1: week 2020-03-08 complete at -20.355247259140015 seconds\n",
      "Index 2: week 2020-03-15 complete at -32.73807430267334 seconds\n",
      "Index 3: week 2020-03-22 complete at -44.563762187957764 seconds\n",
      "Index 4: week 2020-03-29 complete at -58.03542137145996 seconds\n",
      "Index 5: week 2020-04-05 complete at -106.31751132011414 seconds\n",
      "Index 6: week 2020-04-12 complete at -163.2721290588379 seconds\n",
      "Final run time: -163.2722291946411 seconds\n"
     ]
    }
   ],
   "source": [
    "tyler_df = all_tweets(\n",
    "                region =  \"Tyler_Aug30\",\n",
    "                query = ' ',\n",
    "                since = [\"2020-08-30\", \"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\"],\n",
    "                until = [\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\",\"2020-09-06\"],\n",
    "                max_tweets = 2000,\n",
    "                location = \"32.3513, -95.3011\",\n",
    "                radius = \"8mi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tyler_df['text'].str.contains('covid|coronavirus', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-16-ff53110ea0fd>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-ff53110ea0fd>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    32.5007° N, 94.7405° W\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "32.5007° N, 94.7405° W\n",
    "5mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-03-01 complete at -5.210194110870361 seconds\n",
      "Index 1: week 2020-03-08 complete at -10.558757066726685 seconds\n",
      "Index 2: week 2020-03-15 complete at -16.39320397377014 seconds\n",
      "Index 3: week 2020-03-22 complete at -22.334598064422607 seconds\n",
      "Index 4: week 2020-03-29 complete at -30.461591005325317 seconds\n",
      "Index 5: week 2020-04-05 complete at -53.62197518348694 seconds\n",
      "Index 6: week 2020-04-12 complete at -92.676598072052 seconds\n",
      "Final run time: -92.67676901817322 seconds\n"
     ]
    }
   ],
   "source": [
    "longview_df = all_tweets(\n",
    "                region =  \"Longview_Aug30\",\n",
    "                query = ' ',\n",
    "                since = [\"2020-08-30\", \"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\"],\n",
    "                until = [\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\",\"2020-09-06\"],\n",
    "                max_tweets = 2000,\n",
    "                location = \"32.5007, -94.7405\",\n",
    "                radius = \"5mi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longview_df['text'].str.contains('covid|coronavirus', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "33.9137° N, 98.4934° W\n",
    "7mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-08-30 complete at -12.951002836227417 seconds\n",
      "Index 1: week 2020-08-31 complete at -21.91575288772583 seconds\n",
      "Index 2: week 2020-09-01 complete at -38.67661905288696 seconds\n",
      "Index 3: week 2020-09-02 complete at -46.21854782104492 seconds\n",
      "Index 4: week 2020-09-03 complete at -56.51694989204407 seconds\n",
      "Index 5: week 2020-09-04 complete at -64.47669887542725 seconds\n",
      "Index 6: week 2020-09-05 complete at -118.33911609649658 seconds\n",
      "Final run time: -118.33932185173035 seconds\n"
     ]
    }
   ],
   "source": [
    "wichita_df = all_tweets(\n",
    "                region =  \"WichitaFalls_Aug30\",\n",
    "                query = ' ',\n",
    "                since = [\"2020-08-30\", \"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\"],\n",
    "                until = [\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\",\"2020-09-06\"],\n",
    "                max_tweets = 2000,\n",
    "                location = \"33.9137, -98.4934\",\n",
    "                radius = \"7mi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wichita_df['text'].str.contains('covid|coronavirus', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "33.6705° N, 96.6101° W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: week 2020-03-01 complete at -6.2344279289245605 seconds\n",
      "Index 1: week 2020-03-08 complete at -10.435718059539795 seconds\n",
      "Index 2: week 2020-03-15 complete at -14.587816953659058 seconds\n",
      "Index 3: week 2020-03-22 complete at -20.43224000930786 seconds\n",
      "Index 4: week 2020-03-29 complete at -25.42801809310913 seconds\n",
      "Index 5: week 2020-04-05 complete at -50.06227898597717 seconds\n",
      "Index 6: week 2020-04-12 complete at -98.41755080223083 seconds\n",
      "Final run time: -98.41770195960999 seconds\n"
     ]
    }
   ],
   "source": [
    "sherman_df = all_tweets(\n",
    "                region =  \"ShermanDension_Aug30\",\n",
    "                query = ' ',\n",
    "                since = [\"2020-08-30\", \"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\"],\n",
    "                until = [\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\n",
    "                        \"2020-09-04\",\"2020-09-05\",\"2020-09-06\"],\n",
    "                max_tweets = 2000,\n",
    "                location = \"33.6705, -96.6101\",\n",
    "                radius = \"11mi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sherman_df['text'].str.contains('covid|coronavirus', flags=re.IGNORECASE, regex=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sherman_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3080, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wichita_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1744, 9)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longview_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftworth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sheer luck? NO CAUSE COVID IS A LIE! CDC update a few days ago “only 9,210 people have died from Covid in USA.” All other deaths were FROM OTHER CAUSES! It’s been a scam from the beginning. Open the schools, open ALL business at 100% capacity, end the lies @GovAbbott #CovidHoax'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lockdowns and mask mandates are NOT about stopping any disease. They are about CONTROL. And look how many people obediently go along. Standing on dots in stores. Standing in circles at events. Dutifully wearing their face diapers. It’s evil at work. #CovidHoax #MaskOff #Tyranny'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ftworth_df[ftworth_df['text'].str.contains('hoax|qanon', flags=re.IGNORECASE, regex=True)]['text'][10699])\n",
    "print()\n",
    "display(ftworth_df[ftworth_df['text'].str.contains('hoax|qanon', flags=re.IGNORECASE, regex=True)]['text'][11282])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
